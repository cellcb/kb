"""
RAG Demo using LlamaIndex
Simple demonstration of Retrieval-Augmented Generation
"""

import os
from pathlib import Path
from typing import List, Optional, Dict, Any
import argparse

from dotenv import load_dotenv
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt

from llama_index.core import (
    Document,
    Settings,
    StorageContext,
    VectorStoreIndex,
    load_index_from_storage,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.llms import CompletionResponse, CompletionResponseGen, LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback
import requests


class RAGDemo:
    def __init__(self, data_dir: str = "data", persist_dir: str = "storage", 
                 embedding_model: str = "BAAI/bge-small-zh-v1.5"):
        """åˆå§‹åŒ–RAGæ¼”ç¤ºç³»ç»Ÿ"""
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.console = Console()
        self.index = None
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        # load_dotenv()
        
        # é…ç½®LlamaIndexè®¾ç½® - ä½¿ç”¨è‡ªå®šä¹‰DeepSeek LLM
        Settings.llm = OpenAILike(
            model="deepseek-v3-250324",
            api_key="155d5cb5-6b83-4d52-8be8-eb795c72ad44",
            api_base="https://ark.cn-beijing.volces.com/api/v3",
            is_chat_model=True,
            temperature=0.1
        )
        
        # ä½¿ç”¨æœ¬åœ°embeddingæ¨¡å‹
        self._setup_embedding_model(embedding_model)
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
        
    def _setup_embedding_model(self, model_name: str):
        """é…ç½®embeddingæ¨¡å‹"""
        self.console.print(f"[blue]æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_name}[/blue]")
        
        # é¢„å®šä¹‰çš„æ¨¡å‹æ˜ å°„ï¼ŒåŒ…å«ä¸­è‹±æ–‡æ¨¡å‹
        model_info = {
            "BAAI/bge-small-zh-v1.5": "BGEå°å‹ä¸­æ–‡æ¨¡å‹ (æ¨èä¸­æ–‡ä½¿ç”¨)",
            "BAAI/bge-base-zh-v1.5": "BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹ (æ›´å¥½æ•ˆæœä½†æ›´å¤§)",
            "sentence-transformers/all-MiniLM-L6-v2": "è½»é‡çº§è‹±æ–‡æ¨¡å‹ (å¿«é€Ÿ)",
            "sentence-transformers/all-mpnet-base-v2": "é«˜è´¨é‡è‹±æ–‡æ¨¡å‹",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "å¤šè¯­è¨€æ¨¡å‹",
        }
        
        if model_name in model_info:
            self.console.print(f"[green]ä½¿ç”¨æ¨¡å‹: {model_info[model_name]}[/green]")
        
        try:
            Settings.embed_model = HuggingFaceEmbedding(model_name=model_name)
            self.console.print(f"[green]Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
        except Exception as e:
            self.console.print(f"[red]åŠ è½½embeddingæ¨¡å‹å¤±è´¥: {e}[/red]")
            self.console.print("[yellow]å°è¯•ä½¿ç”¨é»˜è®¤çš„è½»é‡çº§æ¨¡å‹...[/yellow]")
            Settings.embed_model = HuggingFaceEmbedding(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
        
    def load_documents(self) -> List[Document]:
        """ä»æ•°æ®ç›®å½•åŠ è½½æ–‡æ¡£"""
        documents = []
        
        if not self.data_dir.exists():
            self.console.print(f"[yellow]æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£...[/yellow]")
            self._create_sample_documents()
        
        for file_path in self.data_dir.glob("*.txt"):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                doc = Document(text=content, metadata={"filename": file_path.name})
                documents.append(doc)
                
        self.console.print(f"[green]åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£[/green]")
        return documents
    
    def _create_sample_documents(self):
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        self.data_dir.mkdir(exist_ok=True)
        
        sample_docs = {
            "machine_learning.txt": """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›å…¶æ€§èƒ½ã€‚
æœºå™¨å­¦ä¹ ç®—æ³•æ„å»ºæ•°å­¦æ¨¡å‹ï¼ŒåŸºäºè®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚

ä¸»è¦ç±»å‹åŒ…æ‹¬ï¼š
1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„
2. æ— ç›‘ç£å­¦ä¹ ï¼šä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼
3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸º

å¸¸è§ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚
            """,
            "deep_learning.txt": """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å…·æœ‰å¤šä¸ªéšè—å±‚çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
è¿™äº›æ·±å±‚ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚

å…³é”®æ¦‚å¿µï¼š
- ç¥ç»ç½‘ç»œï¼šç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆçš„ç½‘ç»œ
- åå‘ä¼ æ’­ï¼šç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ç®—æ³•
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šä¸“é—¨ç”¨äºå¤„ç†å›¾åƒæ•°æ®
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šé€‚åˆå¤„ç†åºåˆ—æ•°æ®
- å˜æ¢å™¨ï¼ˆTransformerï¼‰ï¼šç°ä»£NLPçš„åŸºç¡€æ¶æ„

æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
            """,
            "natural_language_processing.txt": """
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„äº¤å‰é¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

ä¸»è¦ä»»åŠ¡åŒ…æ‹¬ï¼š
- æ–‡æœ¬åˆ†ç±»ï¼šå°†æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«
- å‘½åå®ä½“è¯†åˆ«ï¼šè¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡åç­‰
- æƒ…æ„Ÿåˆ†æï¼šç¡®å®šæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
- æœºå™¨ç¿»è¯‘ï¼šå°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€
- é—®ç­”ç³»ç»Ÿï¼šæ ¹æ®é—®é¢˜ä»æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ
- æ–‡æœ¬æ‘˜è¦ï¼šç”Ÿæˆæ–‡æœ¬çš„ç®€æ´æ‘˜è¦

ç°ä»£NLPå¹¿æ³›ä½¿ç”¨Transformeræ¶æ„ï¼Œå¦‚BERTã€GPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
è¿™äº›æ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒæ¥å®ç°ä¼˜å¼‚æ€§èƒ½ã€‚
            """
        }
        
        for filename, content in sample_docs.items():
            file_path = self.data_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content.strip())
        
        self.console.print(f"[green]åˆ›å»ºäº† {len(sample_docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£[/green]")
    
    def build_index(self) -> VectorStoreIndex:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        self.console.print("[blue]æ„å»ºå‘é‡ç´¢å¼•...[/blue]")
        
        documents = self.load_documents()
        if not documents:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°è¦ç´¢å¼•çš„æ–‡æ¡£")
        
        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(documents, show_progress=True)
        
        # æŒä¹…åŒ–ç´¢å¼•
        self.persist_dir.mkdir(exist_ok=True)
        index.storage_context.persist(persist_dir=str(self.persist_dir))
        
        self.console.print("[green]ç´¢å¼•æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜[/green]")
        return index
    
    def load_index(self) -> Optional[VectorStoreIndex]:
        """åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•"""
        if not self.persist_dir.exists():
            return None
            
        try:
            storage_context = StorageContext.from_defaults(persist_dir=str(self.persist_dir))
            index = load_index_from_storage(storage_context)
            self.console.print("[green]æˆåŠŸåŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•[/green]")
            return index
        except Exception as e:
            self.console.print(f"[yellow]åŠ è½½ç´¢å¼•å¤±è´¥: {e}[/yellow]")
            return None
    
    def get_or_create_index(self) -> VectorStoreIndex:
        """è·å–æˆ–åˆ›å»ºç´¢å¼•"""
        index = self.load_index()
        if index is None:
            index = self.build_index()
        
        self.index = index
        return index
    
    def query(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢RAGç³»ç»Ÿå¹¶è¿”å›ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£"""
        if self.index is None:
            self.get_or_create_index()
        
        query_engine = self.index.as_query_engine(response_mode="compact")
        response = query_engine.query(question)
        
        # æå–æ¥æºæ–‡æ¡£ä¿¡æ¯
        sources = []
        if hasattr(response, 'source_nodes') and response.source_nodes:
            for node in response.source_nodes:
                source_info = {
                    'filename': node.metadata.get('filename', 'æœªçŸ¥æ–‡æ¡£'),
                    'content_preview': node.text[:100] + "..." if len(node.text) > 100 else node.text,
                    'score': getattr(node, 'score', None)
                }
                sources.append(source_info)
        
        return {
            'answer': str(response),
            'sources': sources
        }
    
    def _format_sources(self, sources: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¥æºæ–‡æ¡£ä¿¡æ¯"""
        if not sources:
            return "\n[dim]æœªæ‰¾åˆ°å‚è€ƒæ–‡æ¡£[/dim]"
        
        formatted = "\n[bold yellow]ğŸ“š å‚è€ƒæ–‡æ¡£:[/bold yellow]\n"
        for i, source in enumerate(sources, 1):
            formatted += f"[cyan]{i}. {source['filename']}[/cyan]\n"
            formatted += f"   {source['content_preview']}\n"
            if source.get('score'):
                formatted += f"   [dim]ç›¸å…³åº¦: {source['score']:.3f}[/dim]\n"
        
        return formatted
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©ç•Œé¢"""
        self.console.print(Panel.fit(
            "[bold blue]RAG Demo äº¤äº’å¼é—®ç­”ç³»ç»Ÿ[/bold blue]\n"
            "è¾“å…¥é—®é¢˜æ¥æŸ¥è¯¢çŸ¥è¯†åº“ï¼Œè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º",
            title="æ¬¢è¿ä½¿ç”¨RAG Demo"
        ))
        
        # ç¡®ä¿ç´¢å¼•å·²åŠ è½½
        self.get_or_create_index()
        
        while True:
            try:
                question = Prompt.ask("\n[bold cyan]è¯·è¾“å…¥æ‚¨çš„é—®é¢˜[/bold cyan]")
                
                if question.lower() in ['exit', 'quit', 'é€€å‡º']:
                    self.console.print("[yellow]å†è§ï¼[/yellow]")
                    break
                
                if not question.strip():
                    continue
                
                self.console.print("[blue]æ­£åœ¨æœç´¢ç­”æ¡ˆ...[/blue]")
                result = self.query(question)
                
                # æ˜¾ç¤ºç­”æ¡ˆ
                self.console.print(Panel(
                    result['answer'],
                    title="[bold green]å›ç­”[/bold green]",
                    border_style="green"
                ))
                
                # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
                sources_text = self._format_sources(result['sources'])
                self.console.print(sources_text)
                
            except KeyboardInterrupt:
                self.console.print("\n[yellow]å†è§ï¼[/yellow]")
                break
            except Exception as e:
                self.console.print(f"[red]å‘ç”Ÿé”™è¯¯: {e}[/red]")


def main():
    parser = argparse.ArgumentParser(description="RAG Demo using LlamaIndex")
    parser.add_argument("--data-dir", default="data", help="æ–‡æ¡£æ•°æ®ç›®å½•")
    parser.add_argument("--persist-dir", default="storage", help="ç´¢å¼•å­˜å‚¨ç›®å½•")
    parser.add_argument("--query", help="ç›´æ¥æŸ¥è¯¢è€Œä¸è¿›å…¥äº¤äº’æ¨¡å¼")
    parser.add_argument("--rebuild", action="store_true", help="å¼ºåˆ¶é‡å»ºç´¢å¼•")
    parser.add_argument("--embedding-model", default="BAAI/bge-small-zh-v1.5", 
                        help="Embeddingæ¨¡å‹åç§° (é»˜è®¤: BAAI/bge-small-zh-v1.5)")
    parser.add_argument("--list-models", action="store_true", 
                        help="åˆ—å‡ºæ¨èçš„embeddingæ¨¡å‹")
    parser.add_argument("--test-embedding", action="store_true",
                        help="æµ‹è¯•embeddingæ¨¡å‹ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹é€‰é¡¹
    if args.list_models:
        print("\næ¨èçš„Embeddingæ¨¡å‹:")
        models = {
            "BAAI/bge-small-zh-v1.5": "BGEå°å‹ä¸­æ–‡æ¨¡å‹ (æ¨èä¸­æ–‡ä½¿ç”¨) - è½»é‡å¿«é€Ÿ",
            "BAAI/bge-base-zh-v1.5": "BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹ - æ›´å¥½æ•ˆæœä½†æ›´å¤§",
            "sentence-transformers/all-MiniLM-L6-v2": "è½»é‡çº§è‹±æ–‡æ¨¡å‹ - å¿«é€ŸåŠ è½½",
            "sentence-transformers/all-mpnet-base-v2": "é«˜è´¨é‡è‹±æ–‡æ¨¡å‹ - æœ€ä½³æ•ˆæœ",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "å¤šè¯­è¨€æ”¯æŒæ¨¡å‹",
        }
        for model, desc in models.items():
            print(f"  {model}: {desc}")
        print("\nä½¿ç”¨æ–¹æ³•: --embedding-model MODEL_NAME")
        return
    
    # æµ‹è¯•embeddingæ¨¡å‹
    if args.test_embedding:
        test_embedding_model(args.embedding_model)
        return
    
    # æ£€æŸ¥OpenAI APIå¯†é’¥
    # if not os.getenv("OPENAI_API_KEY"):
    #     print("é”™è¯¯: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    #     print("ä½ å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: OPENAI_API_KEY=your_api_key_here")
    #     return
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGDemo(
        data_dir=args.data_dir, 
        persist_dir=args.persist_dir,
        embedding_model=args.embedding_model
    )
    
    try:
        if args.rebuild:
            rag.console.print("[yellow]å¼ºåˆ¶é‡å»ºç´¢å¼•...[/yellow]")
            rag.build_index()
        
        if args.query:
            # ç›´æ¥æŸ¥è¯¢æ¨¡å¼
            result = rag.query(args.query)
            rag.console.print(Panel(
                result['answer'],
                title="[bold green]å›ç­”[/bold green]",
                border_style="green"
            ))
            
            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            sources_text = rag._format_sources(result['sources'])
            rag.console.print(sources_text)
        else:
            # äº¤äº’æ¨¡å¼
            rag.interactive_chat()
            
    except Exception as e:
        rag.console.print(f"[red]å¯åŠ¨å¤±è´¥: {e}[/red]")


def test_embedding_model(model_name: str):
    """æµ‹è¯•embeddingæ¨¡å‹åŠŸèƒ½"""
    from rich.console import Console
    from rich.panel import Panel
    from rich.table import Table
    import numpy as np
    
    console = Console()
    
    console.print(Panel.fit(
        f"[bold blue]æµ‹è¯• Embedding æ¨¡å‹: {model_name}[/bold blue]",
        title="Embedding æµ‹è¯•"
    ))
    
    try:
        # åŠ è½½æ¨¡å‹
        console.print(f"[blue]æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}[/blue]")
        embed_model = HuggingFaceEmbedding(model_name=model_name)
        console.print("[green]âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
            "è‡ªç„¶è¯­è¨€å¤„ç†ä¸“æ³¨äºè®¡ç®—æœºç†è§£äººç±»è¯­è¨€",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
            "Machine learning is a subset of artificial intelligence"
        ]
        
        console.print("\n[blue]è®¡ç®—æ–‡æœ¬å‘é‡...[/blue]")
        embeddings = []
        
        for i, text in enumerate(test_texts):
            embedding = embed_model.get_text_embedding(text)
            embeddings.append(embedding)
            console.print(f"[green]âœ“ æ–‡æœ¬ {i+1}: å‘é‡ç»´åº¦ {len(embedding)}[/green]")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        console.print("\n[blue]è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦...[/blue]")
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        
        # åˆ›å»ºç›¸ä¼¼åº¦è¡¨æ ¼
        table = Table(title="æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ (å‰3ä¸ªAIç›¸å…³æ–‡æœ¬)")
        table.add_column("æ–‡æœ¬", style="cyan", no_wrap=True)
        for i in range(3):
            table.add_column(f"æ–‡æœ¬{i+1}", justify="center")
        
        ai_texts = test_texts[:3]
        ai_embeddings = embeddings[:3]
        
        for i, text in enumerate(ai_texts):
            row = [f"æ–‡æœ¬{i+1}: {text[:20]}..."]
            for j in range(3):
                similarity = cosine_similarity(ai_embeddings[i], ai_embeddings[j])
                color = "red" if similarity > 0.8 else "yellow" if similarity > 0.6 else "white"
                row.append(f"[{color}]{similarity:.3f}[/{color}]")
            table.add_row(*row)
        
        console.print(table)
        
        # æ˜¾ç¤ºè·¨è¯­è¨€ç›¸ä¼¼åº¦
        if len(embeddings) >= 5:
            cn_ml = embeddings[0]  # ä¸­æ–‡æœºå™¨å­¦ä¹ 
            en_ml = embeddings[4]  # è‹±æ–‡æœºå™¨å­¦ä¹ 
            cross_lang_sim = cosine_similarity(cn_ml, en_ml)
            
            console.print(f"\n[bold yellow]è·¨è¯­è¨€ç›¸ä¼¼åº¦æµ‹è¯•:[/bold yellow]")
            console.print(f"ä¸­æ–‡'æœºå™¨å­¦ä¹ ' vs è‹±æ–‡'Machine learning': [bold green]{cross_lang_sim:.3f}[/bold green]")
        
        console.print(f"\n[bold green]âœ… Embeddingæ¨¡å‹ {model_name} æµ‹è¯•å®Œæˆï¼[/bold green]")
        
    except Exception as e:
        console.print(f"[red]âŒ æµ‹è¯•å¤±è´¥: {e}[/red]")


if __name__ == "__main__":
    main() 