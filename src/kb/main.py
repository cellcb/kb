"""
RAG Demo using LlamaIndex
Simple demonstration of Retrieval-Augmented Generation
"""

import os
from pathlib import Path
from typing import List, Optional, Dict, Any
import argparse
import hashlib
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.progress import Progress, TaskID, SpinnerColumn, TimeElapsedColumn
from rich.table import Table

# PDFå¤„ç†ç›¸å…³å¯¼å…¥
import pypdf
import pdfplumber

# å¯é€‰çš„æ–‡ä»¶ç±»å‹æ£€æµ‹
try:
    import magic
    MAGIC_AVAILABLE = True
except (ImportError, OSError):
    MAGIC_AVAILABLE = False

from llama_index.core import (
    Document,
    Settings,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.llms import CompletionResponse, CompletionResponseGen, LLMMetadata
from llama_index.core.llms.callbacks import llm_completion_callback
import requests

from elasticsearch import Elasticsearch

try:
    from llama_index.vector_stores.elasticsearch import ElasticsearchVectorStore
except ImportError:  # pragma: no cover - fallback for older LlamaIndexç‰ˆæœ¬
    from llama_index.vector_stores.elasticsearch import ElasticsearchStore as ElasticsearchVectorStore


class RAGDemo:
    def __init__(
        self,
        data_dir: str = "data",
        persist_dir: str = "storage",
        embedding_model: str = "BAAI/bge-small-zh-v1.5",
        enable_parallel: bool = True,
        max_workers: int = 2,
        es_url: Optional[str] = None,
        es_index: str = "kb-documents",
        es_user: Optional[str] = None,
        es_password: Optional[str] = None,
    ):
        """åˆå§‹åŒ–RAGæ¼”ç¤ºç³»ç»Ÿ"""
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.console = Console()
        self.index = None
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.enable_parallel = enable_parallel
        self.max_workers = max_workers
        self.cache_dir = self.persist_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # æ–‡ä»¶å¤„ç†ç¼“å­˜
        self.file_cache_path = self.cache_dir / "file_cache.json"
        self.file_cache = self._load_file_cache()
        self.vector_store = None
        self.storage_context = None
        self.es_client = None
        
        # é…ç½®LlamaIndexè®¾ç½® - ä½¿ç”¨è‡ªå®šä¹‰DeepSeek LLM
        Settings.llm = OpenAILike(
            model="deepseek-v3-250324",
            api_key="155d5cb5-6b83-4d52-8be8-eb795c72ad44",
            api_base="https://ark.cn-beijing.volces.com/api/v3",
            is_chat_model=True,
            temperature=0.1
        )
        
        # ä½¿ç”¨æœ¬åœ°embeddingæ¨¡å‹
        self._setup_embedding_model(embedding_model)
        Settings.node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
        
        # Elasticsearch é…ç½®
        self.es_url = es_url or os.getenv("ELASTICSEARCH_URL", "http://localhost:9200")
        self.es_index = os.getenv("ELASTICSEARCH_INDEX", es_index)
        self.es_user = es_user or os.getenv("ELASTICSEARCH_USER")
        self.es_password = es_password or os.getenv("ELASTICSEARCH_PASSWORD")
        self.es_api_key = os.getenv("ELASTICSEARCH_API_KEY")
        verify_env = os.getenv("ELASTICSEARCH_VERIFY_CERTS")
        self.es_verify_certs: Optional[bool] = None
        if verify_env is not None:
            self.es_verify_certs = verify_env.lower() not in {"false", "0", "no"}
        self.es_ca_certs = os.getenv("ELASTICSEARCH_CA_CERTS")
        timeout_env = os.getenv("ELASTICSEARCH_TIMEOUT")
        self.es_timeout = int(timeout_env) if timeout_env and timeout_env.isdigit() else None
        
    def _setup_embedding_model(self, model_name: str):
        """é…ç½®embeddingæ¨¡å‹"""
        self.console.print(f"[blue]æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_name}[/blue]")
        
        # é¢„å®šä¹‰çš„æ¨¡å‹æ˜ å°„ï¼ŒåŒ…å«ä¸­è‹±æ–‡æ¨¡å‹
        model_info = {
            "BAAI/bge-small-zh-v1.5": "BGEå°å‹ä¸­æ–‡æ¨¡å‹ (æ¨èä¸­æ–‡ä½¿ç”¨)",
            "BAAI/bge-base-zh-v1.5": "BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹ (æ›´å¥½æ•ˆæœä½†æ›´å¤§)",
            "sentence-transformers/all-MiniLM-L6-v2": "è½»é‡çº§è‹±æ–‡æ¨¡å‹ (å¿«é€Ÿ)",
            "sentence-transformers/all-mpnet-base-v2": "é«˜è´¨é‡è‹±æ–‡æ¨¡å‹",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "å¤šè¯­è¨€æ¨¡å‹",
        }
        
        if model_name in model_info:
            self.console.print(f"[green]ä½¿ç”¨æ¨¡å‹: {model_info[model_name]}[/green]")
        
        try:
            Settings.embed_model = HuggingFaceEmbedding(model_name=model_name)
            self.console.print(f"[green]Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
        except Exception as e:
            self.console.print(f"[red]åŠ è½½embeddingæ¨¡å‹å¤±è´¥: {e}[/red]")
            self.console.print("[yellow]å°è¯•ä½¿ç”¨é»˜è®¤çš„è½»é‡çº§æ¨¡å‹...[/yellow]")
            Settings.embed_model = HuggingFaceEmbedding(
                                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
    
    def _init_vector_store(self, force: bool = False):
        """åˆå§‹åŒ–æˆ–é‡å»º Elasticsearch å‘é‡å­˜å‚¨"""
        if self.vector_store is not None and self.storage_context is not None and not force:
            return
        
        es_kwargs = {}
        if self.es_user and self.es_password:
            es_kwargs["basic_auth"] = (self.es_user, self.es_password)
        if self.es_api_key:
            es_kwargs["api_key"] = self.es_api_key
        if self.es_verify_certs is not None:
            es_kwargs["verify_certs"] = self.es_verify_certs
        if self.es_ca_certs:
            es_kwargs["ca_certs"] = self.es_ca_certs
        if self.es_timeout:
            es_kwargs["request_timeout"] = self.es_timeout
        
        try:
            self.es_client = Elasticsearch(self.es_url, **es_kwargs)
            self.console.print(f"[blue]è¿æ¥ Elasticsearch: {self.es_url}[/blue]")
        except Exception as exc:
            self.console.print(f"[red]è¿æ¥ Elasticsearch å¤±è´¥: {exc}[/red]")
            raise
        
        try:
            self.es_client.ping()
        except Exception as exc:  # pragma: no cover - ping å¤±è´¥ä¸é˜»æ–­
            self.console.print(f"[yellow]Elasticsearch ping å¤±è´¥: {exc}[/yellow]")
        try:
            self.vector_store = ElasticsearchVectorStore(
                index_name=self.es_index,
                es_url=self.es_url,
                es_user=self.es_user,
                es_password=self.es_password,
                es_api_key=self.es_api_key,
            )
            self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
            self.console.print(f"[green]Elasticsearch å‘é‡ç´¢å¼•å°±ç»ª: {self.es_index}[/green]")
        except Exception as exc:
            self.console.print(f"[red]åˆå§‹åŒ– Elasticsearch å‘é‡å­˜å‚¨å¤±è´¥: {exc}[/red]")
            raise
    
    def _ensure_vector_store(self):
        """ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–"""
        if self.vector_store is None or self.storage_context is None:
            self._init_vector_store()
    
    def _reset_vector_index(self):
        """åˆ é™¤å¹¶é‡å»º Elasticsearch ç´¢å¼•"""
        self._ensure_vector_store()
        
        try:
            delete_method = getattr(self.vector_store, "delete_index", None)
            if callable(delete_method):
                delete_method()
                self.console.print(f"[yellow]å·²æ¸…ç©º Elasticsearch ç´¢å¼• {self.es_index}[/yellow]")
            else:
                self.es_client.indices.delete(index=self.es_index, ignore_unavailable=True)
                self.console.print(f"[yellow]å·²åˆ é™¤ Elasticsearch ç´¢å¼• {self.es_index}[/yellow]")
        except Exception as exc:
            self.console.print(f"[yellow]åˆ é™¤ç´¢å¼•æ—¶é‡åˆ°é—®é¢˜: {exc}[/yellow]")
        finally:
            self._init_vector_store(force=True)
    
    def _refresh_vector_index(self):
        """åˆ·æ–° Elasticsearch ç´¢å¼•"""
        if not self.es_client:
            return
        try:
            self.es_client.indices.refresh(index=self.es_index)
        except Exception as exc:
            self.console.print(f"[yellow]åˆ·æ–° Elasticsearch ç´¢å¼•å¤±è´¥: {exc}[/yellow]")
    
    def _load_file_cache(self) -> Dict[str, Any]:
        """åŠ è½½æ–‡ä»¶å¤„ç†ç¼“å­˜"""
        try:
            if self.file_cache_path.exists():
                with open(self.file_cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.console.print(f"[yellow]ç¼“å­˜åŠ è½½å¤±è´¥: {e}[/yellow]")
        return {}
    
    def _save_file_cache(self):
        """ä¿å­˜æ–‡ä»¶å¤„ç†ç¼“å­˜"""
        try:
            with open(self.file_cache_path, 'w', encoding='utf-8') as f:
                json.dump(self.file_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.console.print(f"[yellow]ç¼“å­˜ä¿å­˜å¤±è´¥: {e}[/yellow]")
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜åˆ¤æ–­"""
        try:
            stat = file_path.stat()
            # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€ä¿®æ”¹æ—¶é—´åˆ›å»ºå“ˆå¸Œ
            content = f"{file_path.name}_{stat.st_size}_{stat.st_mtime}"
            return hashlib.md5(content.encode()).hexdigest()
        except Exception:
            return ""
    
    def _is_file_cached(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¢«ç¼“å­˜ä¸”æœªè¿‡æœŸ"""
        file_hash = self._get_file_hash(file_path)
        if not file_hash:
            return False
        
        cache_key = str(file_path)
        if cache_key in self.file_cache:
            cached_info = self.file_cache[cache_key]
            return cached_info.get('hash') == file_hash
        return False
    
    def _get_cached_content(self, file_path: Path) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ–‡ä»¶å†…å®¹"""
        cache_key = str(file_path)
        if cache_key in self.file_cache:
            return self.file_cache[cache_key].get('content')
        return None
    
    def _cache_file_content(self, file_path: Path, content: str):
        """ç¼“å­˜æ–‡ä»¶å†…å®¹"""
        file_hash = self._get_file_hash(file_path)
        if file_hash:
            cache_key = str(file_path)
            self.file_cache[cache_key] = {
                'hash': file_hash,
                'content': content,
                'timestamp': time.time(),
                'char_count': len(content)
            }
        
    def _detect_file_type(self, file_path: Path) -> str:
        """æ£€æµ‹æ–‡ä»¶çš„çœŸå®ç±»å‹ï¼ˆåŸºäºæ–‡ä»¶å†…å®¹ï¼Œä¸æ˜¯æ‰©å±•åï¼‰"""
        if MAGIC_AVAILABLE:
            try:
                mime_type = magic.from_file(str(file_path), mime=True)
                
                if mime_type == 'application/pdf':
                    return 'pdf'
                elif mime_type.startswith('text/'):
                    return 'txt'
                else:
                    # é™çº§åˆ°æ‰©å±•åæ£€æµ‹
                    return file_path.suffix.lower().lstrip('.')
            except Exception:
                # å¦‚æœmagicæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨æ‰©å±•å
                return file_path.suffix.lower().lstrip('.')
        else:
            # magicä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨æ‰©å±•åæ£€æµ‹
            return file_path.suffix.lower().lstrip('.')
    
    def _extract_pdf_content(self, file_path: Path) -> Optional[str]:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«ç¼“å­˜ã€é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤º"""
        
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        if self._is_file_cached(file_path):
            cached_content = self._get_cached_content(file_path)
            if cached_content:
                self.console.print(f"[green]âœ“ ä½¿ç”¨ç¼“å­˜å†…å®¹: {file_path.name} ({len(cached_content)} å­—ç¬¦)[/green]")
                return cached_content
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°å’ŒåŸºæœ¬æœ‰æ•ˆæ€§
        try:
            file_size = file_path.stat().st_size
            if file_size == 0:
                self.console.print(f"[yellow]è·³è¿‡ç©ºæ–‡ä»¶: {file_path.name}[/yellow]")
                return None
                
            # å¤§æ–‡ä»¶è­¦å‘Š
            if file_size > 50 * 1024 * 1024:  # 50MB
                self.console.print(f"[yellow]è­¦å‘Š: å¤§æ–‡ä»¶ {file_path.name} ({file_size / 1024 / 1024:.1f}MB)ï¼Œå¤„ç†å¯èƒ½è¾ƒæ…¢[/yellow]")
                
        except OSError as e:
            self.console.print(f"[red]æ— æ³•è®¿é—®æ–‡ä»¶ {file_path.name}: {e}[/red]")
            return None
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨pypdfï¼ˆé€Ÿåº¦è¾ƒå¿«ï¼‰
            with self.console.status(f"[blue]ä½¿ç”¨pypdfæå– {file_path.name}..."):
                text = self._extract_with_pypdf(file_path)
                
            if text and text.strip():
                self.console.print(f"[green]âœ“ pypdfæˆåŠŸæå– {len(text)} å­—ç¬¦[/green]")
                # ç¼“å­˜æˆåŠŸæå–çš„å†…å®¹
                self._cache_file_content(file_path, text)
                return text
            
            # å¦‚æœpypdfå¤±è´¥æˆ–ç»“æœä¸ºç©ºï¼Œå°è¯•pdfplumber
            self.console.print(f"[yellow]pypdfæå–ç»“æœä¸ºç©ºï¼Œå°è¯•pdfplumber: {file_path.name}[/yellow]")
            
            with self.console.status(f"[blue]ä½¿ç”¨pdfplumberæå– {file_path.name}..."):
                text = self._extract_with_pdfplumber(file_path)
                
            if text and text.strip():
                self.console.print(f"[green]âœ“ pdfplumberæˆåŠŸæå– {len(text)} å­—ç¬¦[/green]")
                # ç¼“å­˜æˆåŠŸæå–çš„å†…å®¹
                self._cache_file_content(file_path, text)
                return text
            else:
                self.console.print(f"[yellow]âš  PDFæ–‡ä»¶ {file_path.name} å¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–æŸå[/yellow]")
                return None
            
        except MemoryError:
            self.console.print(f"[red]âŒ å†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†å¤§æ–‡ä»¶: {file_path.name}[/red]")
            return None
        except PermissionError:
            self.console.print(f"[red]âŒ æƒé™ä¸è¶³ï¼Œæ— æ³•è¯»å–æ–‡ä»¶: {file_path.name}[/red]")
            return None
        except Exception as e:
            error_type = type(e).__name__
            self.console.print(f"[red]âŒ PDFæå–å¤±è´¥ {file_path.name} ({error_type}): {str(e)[:100]}[/red]")
            return None
    
    def _extract_with_pypdf(self, file_path: Path) -> str:
        """ä½¿ç”¨pypdfæå–PDFæ–‡æœ¬ï¼ŒåŒ…å«é¡µé¢çº§è¿›åº¦æ˜¾ç¤º"""
        text = ""
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                if total_pages == 0:
                    self.console.print(f"[yellow]PDFæ–‡ä»¶ {file_path.name} æ²¡æœ‰é¡µé¢[/yellow]")
                    return ""
                
                # å¯¹äºå¤šé¡µPDFæ˜¾ç¤ºè¿›åº¦
                if total_pages > 5:
                    with Progress(
                        SpinnerColumn(),
                        "[progress.description]{task.description}",
                        "[progress.percentage]{task.percentage:>3.0f}%",
                        TimeElapsedColumn(),
                        console=self.console
                    ) as progress:
                        task = progress.add_task(f"æå– {file_path.name}", total=total_pages)
                        
                        for page_num, page in enumerate(pdf_reader.pages):
                            try:
                                page_text = page.extract_text()
                                if page_text and page_text.strip():
                                    text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                                progress.advance(task)
                            except Exception as e:
                                self.console.print(f"[yellow]è·³è¿‡ç¬¬{page_num + 1}é¡µ: {str(e)[:50]}[/yellow]")
                                progress.advance(task)
                                continue
                else:
                    # å°‘é¡µæ•°PDFç›´æ¥å¤„ç†
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                        except Exception as e:
                            self.console.print(f"[yellow]è·³è¿‡ç¬¬{page_num + 1}é¡µ: {str(e)[:50]}[/yellow]")
                            continue
                            
        except Exception as e:
            raise Exception(f"pypdfè¯»å–å¤±è´¥: {e}")
            
        return text.strip()
    
    def _extract_with_pdfplumber(self, file_path: Path) -> str:
        """ä½¿ç”¨pdfplumberæå–PDFæ–‡æœ¬ï¼ˆå¤„ç†å¤æ‚å¸ƒå±€ï¼‰ï¼ŒåŒ…å«è¿›åº¦æ˜¾ç¤º"""
        text = ""
        
        try:
            with pdfplumber.open(file_path) as pdf:
                total_pages = len(pdf.pages)
                
                if total_pages == 0:
                    self.console.print(f"[yellow]PDFæ–‡ä»¶ {file_path.name} æ²¡æœ‰é¡µé¢[/yellow]")
                    return ""
                
                # å¯¹äºå¤šé¡µPDFæ˜¾ç¤ºè¿›åº¦
                if total_pages > 3:
                    with Progress(
                        SpinnerColumn(),
                        "[progress.description]{task.description}",
                        "[progress.percentage]{task.percentage:>3.0f}%",
                        TimeElapsedColumn(),
                        console=self.console
                    ) as progress:
                        task = progress.add_task(f"pdfplumberå¤„ç† {file_path.name}", total=total_pages)
                        
                        for page_num, page in enumerate(pdf.pages):
                            try:
                                page_text = page.extract_text()
                                if page_text and page_text.strip():
                                    text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                                progress.advance(task)
                            except Exception as e:
                                self.console.print(f"[yellow]è·³è¿‡ç¬¬{page_num + 1}é¡µ: {str(e)[:50]}[/yellow]")
                                progress.advance(task)
                                continue
                else:
                    # å°‘é¡µæ•°PDFç›´æ¥å¤„ç†
                    for page_num, page in enumerate(pdf.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                        except Exception as e:
                            self.console.print(f"[yellow]è·³è¿‡ç¬¬{page_num + 1}é¡µ: {str(e)[:50]}[/yellow]")
                            continue
                            
        except Exception as e:
            raise Exception(f"pdfplumberå¤„ç†å¤±è´¥: {e}")
            
        return text.strip()
    
    def _process_single_file(self, file_path: Path) -> Optional[Document]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›Documentå¯¹è±¡æˆ–None"""
        detected_type = self._detect_file_type(file_path)
        
        try:
            if detected_type == 'pdf':
                content = self._extract_pdf_content(file_path)
                if content:
                    return Document(
                        text=content, 
                        metadata={
                            "filename": file_path.name,
                            "file_type": "pdf",
                            "file_path": str(file_path),
                            "file_size": file_path.stat().st_size,
                            "char_count": len(content)
                        }
                    )
                    
            elif detected_type == 'txt':
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if content.strip():
                            return Document(
                                text=content, 
                                metadata={
                                    "filename": file_path.name,
                                    "file_type": "txt",
                                    "file_path": str(file_path),
                                    "file_size": file_path.stat().st_size,
                                    "char_count": len(content)
                                }
                            )
                except UnicodeDecodeError:
                    self.console.print(f"[yellow]æ–‡æœ¬æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè·³è¿‡: {file_path.name}[/yellow]")
                    
        except Exception as e:
            self.console.print(f"[red]å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {str(e)[:100]}[/red]")
        
        return None
        
    def load_documents(self) -> List[Document]:
        """ä»æ•°æ®ç›®å½•åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒtxtå’Œpdfæ ¼å¼ï¼‰ï¼ŒåŒ…å«è¯¦ç»†è¿›åº¦å’Œç»Ÿè®¡"""
        documents = []
        
        if not self.data_dir.exists():
            self.console.print(f"[yellow]æ•°æ®ç›®å½• {self.data_dir} ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£...[/yellow]")
            self._create_sample_documents()
        
        # é¦–å…ˆæ‰«ææ‰€æœ‰æ–‡ä»¶è·å–æ€»æ•°
        all_files = [f for f in self.data_dir.iterdir() if f.is_file()]
        if not all_files:
            self.console.print("[yellow]æ•°æ®ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶[/yellow]")
            return documents
        
        self.console.print(f"[blue]å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...[/blue]")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'processed': 0,
            'skipped': 0,
            'pdf_files': 0,
            'txt_files': 0,
            'other_files': 0,
            'total_chars': 0,
            'failed': 0
        }
        
        # å¤„ç†æ–‡ä»¶ - æ”¯æŒå¹¶è¡Œæˆ–ä¸²è¡Œ
        start_time = time.time()
        
        if self.enable_parallel and len(all_files) > 1:
            documents = self._process_files_parallel(all_files, stats)
        else:
            documents = self._process_files_sequential(all_files, stats)
        
        # ä¿å­˜ç¼“å­˜
        self._save_file_cache()
        
        processing_time = time.time() - start_time
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        self._display_processing_stats(stats, processing_time)
        return documents
    
    def _process_files_sequential(self, all_files: List[Path], stats: Dict[str, int]) -> List[Document]:
        """ä¸²è¡Œå¤„ç†æ–‡ä»¶"""
        documents = []
        
        with Progress(
            SpinnerColumn(),
            "[progress.description]{task.description}",
            "[progress.percentage]{task.percentage:>3.0f}%",
            "[progress.completed]{task.completed}/{task.total}",
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            
            main_task = progress.add_task("ä¸²è¡Œå¤„ç†æ–‡æ¡£", total=len(all_files))
            
            for file_path in all_files:
                progress.update(main_task, description=f"å¤„ç† {file_path.name}")
                
                doc = self._process_single_file(file_path)
                if doc:
                    documents.append(doc)
                    file_type = doc.metadata.get('file_type', 'unknown')
                    if file_type == 'pdf':
                        stats['pdf_files'] += 1
                    elif file_type == 'txt':
                        stats['txt_files'] += 1
                    
                    stats['processed'] += 1
                    stats['total_chars'] += doc.metadata.get('char_count', 0)
                else:
                    # ç¡®å®šè·³è¿‡åŸå› 
                    detected_type = self._detect_file_type(file_path)
                    if detected_type in ['pdf', 'txt']:
                        stats['failed'] += 1
                    else:
                        stats['other_files'] += 1
                        stats['skipped'] += 1
                
                progress.advance(main_task)
        
        return documents
    
    def _process_files_parallel(self, all_files: List[Path], stats: Dict[str, int]) -> List[Document]:
        """å¹¶è¡Œå¤„ç†æ–‡ä»¶"""
        documents = []
        
        self.console.print(f"[blue]ä½¿ç”¨å¹¶è¡Œå¤„ç† (æœ€å¤§ {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹)[/blue]")
        
        with Progress(
            SpinnerColumn(),
            "[progress.description]{task.description}",
            "[progress.percentage]{task.percentage:>3.0f}%",
            "[progress.completed]{task.completed}/{task.total}",
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            
            main_task = progress.add_task("å¹¶è¡Œå¤„ç†æ–‡æ¡£", total=len(all_files))
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {
                    executor.submit(self._process_single_file, file_path): file_path 
                    for file_path in all_files
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    progress.update(main_task, description=f"å®Œæˆ {file_path.name}")
                    
                    try:
                        doc = future.result()
                        if doc:
                            documents.append(doc)
                            file_type = doc.metadata.get('file_type', 'unknown')
                            if file_type == 'pdf':
                                stats['pdf_files'] += 1
                            elif file_type == 'txt':
                                stats['txt_files'] += 1
                            
                            stats['processed'] += 1
                            stats['total_chars'] += doc.metadata.get('char_count', 0)
                        else:
                            # ç¡®å®šè·³è¿‡åŸå› 
                            detected_type = self._detect_file_type(file_path)
                            if detected_type in ['pdf', 'txt']:
                                stats['failed'] += 1
                            else:
                                stats['other_files'] += 1
                                stats['skipped'] += 1
                                
                    except Exception as e:
                        self.console.print(f"[red]å¹¶è¡Œå¤„ç†å¤±è´¥ {file_path.name}: {e}[/red]")
                        stats['failed'] += 1
                    
                    progress.advance(main_task)
        
        return documents
    
    def _display_processing_stats(self, stats: Dict[str, int], processing_time: float):
        """æ˜¾ç¤ºæ–‡æ¡£å¤„ç†ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«æ€§èƒ½æ•°æ®"""
        table = Table(title="æ–‡æ¡£å¤„ç†ç»Ÿè®¡", show_header=True, header_style="bold magenta")
        table.add_column("é¡¹ç›®", style="cyan", no_wrap=True)
        table.add_column("æ•°é‡", justify="right", style="green")
        table.add_column("è¯´æ˜", style="dim")
        
        table.add_row("æˆåŠŸå¤„ç†", str(stats['processed']), "âœ“ å·²åŠ è½½åˆ°å‘é‡æ•°æ®åº“")
        table.add_row("PDFæ–‡ä»¶", str(stats['pdf_files']), "é€šè¿‡pypdf/pdfplumberæå–")
        table.add_row("æ–‡æœ¬æ–‡ä»¶", str(stats['txt_files']), "ç›´æ¥è¯»å–")
        table.add_row("è·³è¿‡æ–‡ä»¶", str(stats['skipped']), "ç©ºæ–‡ä»¶æˆ–ä¸æ”¯æŒæ ¼å¼")
        table.add_row("å¤±è´¥æ–‡ä»¶", str(stats['failed']), "âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™")
        table.add_row("æ€»å­—ç¬¦æ•°", f"{stats['total_chars']:,}", "æå–çš„æ–‡æœ¬æ€»é•¿åº¦")
        table.add_row("å¤„ç†æ—¶é—´", f"{processing_time:.2f}ç§’", f"{'å¹¶è¡Œ' if self.enable_parallel else 'ä¸²è¡Œ'}å¤„ç†")
        
        self.console.print(table)
        
        # æ€§èƒ½æŒ‡æ ‡
        if stats['processed'] > 0:
            avg_chars = stats['total_chars'] // stats['processed']
            chars_per_sec = stats['total_chars'] / processing_time if processing_time > 0 else 0
            files_per_sec = stats['processed'] / processing_time if processing_time > 0 else 0
            
            self.console.print(f"[green]âœ… å¹³å‡æ¯ä¸ªæ–‡æ¡£ {avg_chars:,} å­—ç¬¦[/green]")
            self.console.print(f"[blue]ğŸ“Š å¤„ç†é€Ÿåº¦: {files_per_sec:.1f} æ–‡ä»¶/ç§’, {chars_per_sec:,.0f} å­—ç¬¦/ç§’[/blue]")
        
        # ç¼“å­˜ç»Ÿè®¡
        cached_files = sum(1 for v in self.file_cache.values() if 'content' in v)
        if cached_files > 0:
            self.console.print(f"[cyan]ğŸ’¾ ç¼“å­˜ä¸­æœ‰ {cached_files} ä¸ªæ–‡ä»¶ï¼Œä¸‹æ¬¡å¤„ç†å°†æ›´å¿«[/cyan]")
        
        if stats['failed'] > 0:
            self.console.print(f"[yellow]âš ï¸  {stats['failed']} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§[/yellow]")
    
    def _create_sample_documents(self):
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        self.data_dir.mkdir(exist_ok=True)
        
        sample_docs = {
            "machine_learning.txt": """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›å…¶æ€§èƒ½ã€‚
æœºå™¨å­¦ä¹ ç®—æ³•æ„å»ºæ•°å­¦æ¨¡å‹ï¼ŒåŸºäºè®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚

ä¸»è¦ç±»å‹åŒ…æ‹¬ï¼š
1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„
2. æ— ç›‘ç£å­¦ä¹ ï¼šä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼
3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸º

å¸¸è§ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚
            """,
            "deep_learning.txt": """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å…·æœ‰å¤šä¸ªéšè—å±‚çš„äººå·¥ç¥ç»ç½‘ç»œã€‚
è¿™äº›æ·±å±‚ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚

å…³é”®æ¦‚å¿µï¼š
- ç¥ç»ç½‘ç»œï¼šç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆçš„ç½‘ç»œ
- åå‘ä¼ æ’­ï¼šç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ç®—æ³•
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šä¸“é—¨ç”¨äºå¤„ç†å›¾åƒæ•°æ®
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šé€‚åˆå¤„ç†åºåˆ—æ•°æ®
- å˜æ¢å™¨ï¼ˆTransformerï¼‰ï¼šç°ä»£NLPçš„åŸºç¡€æ¶æ„

æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
            """,
            "natural_language_processing.txt": """
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„äº¤å‰é¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

ä¸»è¦ä»»åŠ¡åŒ…æ‹¬ï¼š
- æ–‡æœ¬åˆ†ç±»ï¼šå°†æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«
- å‘½åå®ä½“è¯†åˆ«ï¼šè¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡åç­‰
- æƒ…æ„Ÿåˆ†æï¼šç¡®å®šæ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
- æœºå™¨ç¿»è¯‘ï¼šå°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€
- é—®ç­”ç³»ç»Ÿï¼šæ ¹æ®é—®é¢˜ä»æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ
- æ–‡æœ¬æ‘˜è¦ï¼šç”Ÿæˆæ–‡æœ¬çš„ç®€æ´æ‘˜è¦

ç°ä»£NLPå¹¿æ³›ä½¿ç”¨Transformeræ¶æ„ï¼Œå¦‚BERTã€GPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
è¿™äº›æ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¾®è°ƒæ¥å®ç°ä¼˜å¼‚æ€§èƒ½ã€‚
            """
        }
        
        for filename, content in sample_docs.items():
            file_path = self.data_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content.strip())
        
        self.console.print(f"[green]åˆ›å»ºäº† {len(sample_docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£[/green]")
    
    def build_index(self, reset_existing: bool = False) -> VectorStoreIndex:
        """æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå†™å…¥ Elasticsearchï¼‰"""
        self.console.print("[blue]æ„å»ºå‘é‡ç´¢å¼•ï¼ˆElasticsearchï¼‰...[/blue]")
        self._ensure_vector_store()
        
        documents = self.load_documents()
        if not documents:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°è¦ç´¢å¼•çš„æ–‡æ¡£")
        
        if reset_existing:
            self._reset_vector_index()
        
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=self.storage_context,
            show_progress=True,
        )
        self._refresh_vector_index()
        
        self.console.print("[green]Elasticsearch å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ[/green]")
        self.index = index
        return index
    
    def load_index(self) -> Optional[VectorStoreIndex]:
        """å°è¯•åŠ è½½ç°æœ‰çš„ Elasticsearch å‘é‡ç´¢å¼•"""
        self._ensure_vector_store()
        
        try:
            if not self.es_client.indices.exists(index=self.es_index):
                self.console.print("[yellow]Elasticsearch ç´¢å¼•ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°æ„å»º[/yellow]")
                return None
        except Exception as exc:
            self.console.print(f"[yellow]æ£€æµ‹ Elasticsearch ç´¢å¼•å¤±è´¥: {exc}[/yellow]")
            return None
        
        try:
            index = VectorStoreIndex.from_vector_store(
                self.vector_store,
                storage_context=self.storage_context,
            )
            self.console.print("[green]æˆåŠŸè¿æ¥åˆ° Elasticsearch å‘é‡ç´¢å¼•[/green]")
            self.index = index
            return index
        except Exception as exc:
            self.console.print(f"[yellow]åŠ è½½ Elasticsearch ç´¢å¼•å¤±è´¥: {exc}[/yellow]")
            return None
    
    def get_or_create_index(self) -> VectorStoreIndex:
        """è·å–æˆ–åˆ›å»ºç´¢å¼•"""
        index = self.load_index()
        if index is None:
            index = self.build_index()
        
        self.index = index
        return index
    
    def query(self, question: str) -> Dict[str, Any]:
        """æŸ¥è¯¢RAGç³»ç»Ÿå¹¶è¿”å›ç­”æ¡ˆå’Œæ¥æºæ–‡æ¡£"""
        self._ensure_vector_store()
        if self.index is None:
            self.get_or_create_index()
        
        query_engine = self.index.as_query_engine(response_mode="compact")
        response = query_engine.query(question)
        
        # æå–æ¥æºæ–‡æ¡£ä¿¡æ¯
        sources = []
        if hasattr(response, 'source_nodes') and response.source_nodes:
            for node in response.source_nodes:
                source_info = {
                    'filename': node.metadata.get('filename', 'æœªçŸ¥æ–‡æ¡£'),
                    'content_preview': node.text[:100] + "..." if len(node.text) > 100 else node.text,
                    'score': getattr(node, 'score', None)
                }
                sources.append(source_info)
        
        return {
            'answer': str(response),
            'sources': sources
        }
    
    def _format_sources(self, sources: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æ¥æºæ–‡æ¡£ä¿¡æ¯"""
        if not sources:
            return "\n[dim]æœªæ‰¾åˆ°å‚è€ƒæ–‡æ¡£[/dim]"
        
        formatted = "\n[bold yellow]ğŸ“š å‚è€ƒæ–‡æ¡£:[/bold yellow]\n"
        for i, source in enumerate(sources, 1):
            formatted += f"[cyan]{i}. {source['filename']}[/cyan]\n"
            formatted += f"   {source['content_preview']}\n"
            if source.get('score'):
                formatted += f"   [dim]ç›¸å…³åº¦: {source['score']:.3f}[/dim]\n"
        
        return formatted
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©ç•Œé¢"""
        self.console.print(Panel.fit(
            "[bold blue]RAG Demo äº¤äº’å¼é—®ç­”ç³»ç»Ÿ[/bold blue]\n"
            "è¾“å…¥é—®é¢˜æ¥æŸ¥è¯¢çŸ¥è¯†åº“ï¼Œè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º",
            title="æ¬¢è¿ä½¿ç”¨RAG Demo"
        ))
        
        # ç¡®ä¿ç´¢å¼•å·²åŠ è½½
        self.get_or_create_index()
        
        while True:
            try:
                question = Prompt.ask("\n[bold cyan]è¯·è¾“å…¥æ‚¨çš„é—®é¢˜[/bold cyan]")
                
                if question.lower() in ['exit', 'quit', 'é€€å‡º']:
                    self.console.print("[yellow]å†è§ï¼[/yellow]")
                    break
                
                if not question.strip():
                    continue
                
                self.console.print("[blue]æ­£åœ¨æœç´¢ç­”æ¡ˆ...[/blue]")
                result = self.query(question)
                
                # æ˜¾ç¤ºç­”æ¡ˆ
                self.console.print(Panel(
                    result['answer'],
                    title="[bold green]å›ç­”[/bold green]",
                    border_style="green"
                ))
                
                # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
                sources_text = self._format_sources(result['sources'])
                self.console.print(sources_text)
                
            except KeyboardInterrupt:
                self.console.print("\n[yellow]å†è§ï¼[/yellow]")
                break
            except Exception as e:
                self.console.print(f"[red]å‘ç”Ÿé”™è¯¯: {e}[/red]")


def main():
    parser = argparse.ArgumentParser(description="RAG Demo using LlamaIndex")
    parser.add_argument("--data-dir", default="data", help="æ–‡æ¡£æ•°æ®ç›®å½•")
    parser.add_argument("--persist-dir", default="storage", help="ç´¢å¼•å­˜å‚¨ç›®å½•")
    parser.add_argument("--query", help="ç›´æ¥æŸ¥è¯¢è€Œä¸è¿›å…¥äº¤äº’æ¨¡å¼")
    parser.add_argument("--rebuild", action="store_true", help="å¼ºåˆ¶é‡å»ºç´¢å¼•")
    parser.add_argument("--embedding-model", default="BAAI/bge-small-zh-v1.5", 
                        help="Embeddingæ¨¡å‹åç§° (é»˜è®¤: BAAI/bge-small-zh-v1.5)")
    parser.add_argument("--list-models", action="store_true", 
                        help="åˆ—å‡ºæ¨èçš„embeddingæ¨¡å‹")
    parser.add_argument("--test-embedding", action="store_true",
                        help="æµ‹è¯•embeddingæ¨¡å‹ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰")
    parser.add_argument("--disable-parallel", action="store_true",
                        help="ç¦ç”¨å¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ä¸²è¡Œæ¨¡å¼")
    parser.add_argument("--max-workers", type=int, default=2,
                        help="å¹¶è¡Œå¤„ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 2)")
    parser.add_argument("--es-url", help="Elasticsearch åœ°å€ (é»˜è®¤è¯»å–ç¯å¢ƒå˜é‡ ELASTICSEARCH_URL)")
    parser.add_argument("--es-index", default="kb-documents",
                        help="Elasticsearch ç´¢å¼•åç§° (é»˜è®¤: kb-documents)")
    parser.add_argument("--es-user", help="Elasticsearch åŸºæœ¬è®¤è¯ç”¨æˆ·å")
    parser.add_argument("--es-password", help="Elasticsearch åŸºæœ¬è®¤è¯å¯†ç ")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹é€‰é¡¹
    if args.list_models:
        print("\næ¨èçš„Embeddingæ¨¡å‹:")
        models = {
            "BAAI/bge-small-zh-v1.5": "BGEå°å‹ä¸­æ–‡æ¨¡å‹ (æ¨èä¸­æ–‡ä½¿ç”¨) - è½»é‡å¿«é€Ÿ",
            "BAAI/bge-base-zh-v1.5": "BGEåŸºç¡€ä¸­æ–‡æ¨¡å‹ - æ›´å¥½æ•ˆæœä½†æ›´å¤§",
            "sentence-transformers/all-MiniLM-L6-v2": "è½»é‡çº§è‹±æ–‡æ¨¡å‹ - å¿«é€ŸåŠ è½½",
            "sentence-transformers/all-mpnet-base-v2": "é«˜è´¨é‡è‹±æ–‡æ¨¡å‹ - æœ€ä½³æ•ˆæœ",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": "å¤šè¯­è¨€æ”¯æŒæ¨¡å‹",
        }
        for model, desc in models.items():
            print(f"  {model}: {desc}")
        print("\nä½¿ç”¨æ–¹æ³•: --embedding-model MODEL_NAME")
        return
    
    # æµ‹è¯•embeddingæ¨¡å‹
    if args.test_embedding:
        test_embedding_model(args.embedding_model)
        return
    
    # æ£€æŸ¥OpenAI APIå¯†é’¥
    # if not os.getenv("OPENAI_API_KEY"):
    #     print("é”™è¯¯: è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    #     print("ä½ å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: OPENAI_API_KEY=your_api_key_here")
    #     return
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGDemo(
        data_dir=args.data_dir, 
        persist_dir=args.persist_dir,
        embedding_model=args.embedding_model,
        enable_parallel=not args.disable_parallel,
        max_workers=args.max_workers,
        es_url=args.es_url,
        es_index=args.es_index,
        es_user=args.es_user,
        es_password=args.es_password,
    )
    
    try:
        if args.rebuild:
            rag.console.print("[yellow]å¼ºåˆ¶é‡å»ºç´¢å¼•...[/yellow]")
            rag.build_index(reset_existing=True)
        
        if args.query:
            # ç›´æ¥æŸ¥è¯¢æ¨¡å¼
            result = rag.query(args.query)
            rag.console.print(Panel(
                result['answer'],
                title="[bold green]å›ç­”[/bold green]",
                border_style="green"
            ))
            
            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            sources_text = rag._format_sources(result['sources'])
            rag.console.print(sources_text)
        else:
            # äº¤äº’æ¨¡å¼
            rag.interactive_chat()
            
    except Exception as e:
        rag.console.print(f"[red]å¯åŠ¨å¤±è´¥: {e}[/red]")


def test_embedding_model(model_name: str):
    """æµ‹è¯•embeddingæ¨¡å‹åŠŸèƒ½"""
    from rich.console import Console
    from rich.panel import Panel
    from rich.table import Table
    import numpy as np
    
    console = Console()
    
    console.print(Panel.fit(
        f"[bold blue]æµ‹è¯• Embedding æ¨¡å‹: {model_name}[/bold blue]",
        title="Embedding æµ‹è¯•"
    ))
    
    try:
        # åŠ è½½æ¨¡å‹
        console.print(f"[blue]æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}[/blue]")
        embed_model = HuggingFaceEmbedding(model_name=model_name)
        console.print("[green]âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ[/green]")
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
            "è‡ªç„¶è¯­è¨€å¤„ç†ä¸“æ³¨äºè®¡ç®—æœºç†è§£äººç±»è¯­è¨€",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
            "Machine learning is a subset of artificial intelligence"
        ]
        
        console.print("\n[blue]è®¡ç®—æ–‡æœ¬å‘é‡...[/blue]")
        embeddings = []
        
        for i, text in enumerate(test_texts):
            embedding = embed_model.get_text_embedding(text)
            embeddings.append(embedding)
            console.print(f"[green]âœ“ æ–‡æœ¬ {i+1}: å‘é‡ç»´åº¦ {len(embedding)}[/green]")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        console.print("\n[blue]è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦...[/blue]")
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
        
        # åˆ›å»ºç›¸ä¼¼åº¦è¡¨æ ¼
        table = Table(title="æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ (å‰3ä¸ªAIç›¸å…³æ–‡æœ¬)")
        table.add_column("æ–‡æœ¬", style="cyan", no_wrap=True)
        for i in range(3):
            table.add_column(f"æ–‡æœ¬{i+1}", justify="center")
        
        ai_texts = test_texts[:3]
        ai_embeddings = embeddings[:3]
        
        for i, text in enumerate(ai_texts):
            row = [f"æ–‡æœ¬{i+1}: {text[:20]}..."]
            for j in range(3):
                similarity = cosine_similarity(ai_embeddings[i], ai_embeddings[j])
                color = "red" if similarity > 0.8 else "yellow" if similarity > 0.6 else "white"
                row.append(f"[{color}]{similarity:.3f}[/{color}]")
            table.add_row(*row)
        
        console.print(table)
        
        # æ˜¾ç¤ºè·¨è¯­è¨€ç›¸ä¼¼åº¦
        if len(embeddings) >= 5:
            cn_ml = embeddings[0]  # ä¸­æ–‡æœºå™¨å­¦ä¹ 
            en_ml = embeddings[4]  # è‹±æ–‡æœºå™¨å­¦ä¹ 
            cross_lang_sim = cosine_similarity(cn_ml, en_ml)
            
            console.print(f"\n[bold yellow]è·¨è¯­è¨€ç›¸ä¼¼åº¦æµ‹è¯•:[/bold yellow]")
            console.print(f"ä¸­æ–‡'æœºå™¨å­¦ä¹ ' vs è‹±æ–‡'Machine learning': [bold green]{cross_lang_sim:.3f}[/bold green]")
        
        console.print(f"\n[bold green]âœ… Embeddingæ¨¡å‹ {model_name} æµ‹è¯•å®Œæˆï¼[/bold green]")
        
    except Exception as e:
        console.print(f"[red]âŒ æµ‹è¯•å¤±è´¥: {e}[/red]")


if __name__ == "__main__":
    main() 
